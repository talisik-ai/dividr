import * as path from 'path';
import {
  AudioProcessingContext,
  AudioTrimResult,
  CategorizedInputs,
  CommandParts,
  InputCategory,
  TrackInfo,
  VideoEditJob,
  VideoProcessingContext,
} from '../schema/ffmpegConfig';
import { handleFilterComplex } from './handleFilterComplex';
import {
  getHardwareAcceleration,
  getSpecificHardwareAcceleration,
  type HardwareAcceleration,
} from './hardwareAccelerationDetector';
import { handleTimelineProcessing } from './timelineBuilder';

const VIDEO_DEFAULTS = {
  SIZE: { width: 1920, height: 1080 },
  FPS: 30,
  DUMMY_DURATION: 0.1,
} as const;

const AUDIO_DEFAULTS = {
  CHANNEL_LAYOUT: 'stereo',
  SAMPLE_RATE: 48000,
} as const;

const GAP_MARKER = '__GAP__' as const;

function escapePath(filePath: string) {
  return filePath;
}

const FILE_EXTENSIONS = {
  VIDEO: /\.(mp4|mov|mkv|avi|webm)$/i,
  AUDIO: /\.(mp3|wav|aac|flac)$/i,
  IMAGE: /\.(png|jpg|jpeg|gif|bmp|tiff|webp)$/i,
} as const;

const ENCODING_DEFAULTS = {
  VIDEO_CODEC: 'libx264',
  AUDIO_CODEC: 'aac',
} as const;

/**
 * Gets the appropriate hardware acceleration settings for a job
 */
async function getHardwareAccelerationForJob(
  job: VideoEditJob,
  ffmpegPath?: string,
): Promise<HardwareAcceleration | null> {
  // If hardware acceleration is explicitly disabled, return null
  if (
    job.operations.useHardwareAcceleration === false ||
    job.operations.hwaccelType === 'none'
  ) {
    console.log('üö´ Hardware acceleration disabled by job config');
    return null;
  }
  console.log('üéÆ Hardware acceleration is enabled');

  try {
    // If specific hardware type is requested
    if (job.operations.hwaccelType && job.operations.hwaccelType !== 'auto') {
      const specificHW = await getSpecificHardwareAcceleration(
        job.operations.hwaccelType,
        ffmpegPath,
      );
      if (specificHW) {
        console.log(
          `üéÆ Using requested hardware acceleration: ${job.operations.hwaccelType.toUpperCase()}`,
        );
        return specificHW;
      } else {
        console.warn(
          `‚ö†Ô∏è Requested hardware acceleration "${job.operations.hwaccelType}" not available, falling back to software`,
        );
        return null;
      }
    }

    // Auto-detect best hardware acceleration
    const detection = await getHardwareAcceleration(ffmpegPath);
    if (detection.primary) {
      console.log(
        `üéÆ Auto-detected hardware acceleration: ${detection.primary.type.toUpperCase()}`,
      );
      return detection.primary;
    }

    console.log(
      '‚ö†Ô∏è No hardware acceleration available, using software encoding',
    );
    return null;
  } catch (error) {
    console.error('‚ùå Error detecting hardware acceleration:', error);
    return null;
  }
}

// -------------------------
// Input Processing Utilities
// -------------------------

/**
 * Helper to get path from input (string or TrackInfo)
 */
function getInputPath(input: string | TrackInfo): string {
  return typeof input === 'string' ? input : input.path;
}

/**
 * Helper to get track info from input
 */
function getTrackInfo(input: string | TrackInfo): TrackInfo {
  return typeof input === 'string' ? { path: input } : input;
}

/**
 * Helper to check if input is a gap marker
 */
function isGapInput(path: string): boolean {
  return path === GAP_MARKER;
}

/**
 * Categorizes inputs into video and audio arrays with proper indexing
 * @param inputs - Array of video edit job inputs
 * @returns Categorized inputs with proper file indexing
 */
function categorizeInputs(inputs: (string | TrackInfo)[]): CategorizedInputs {
  const videoInputs: InputCategory[] = [];
  const audioInputs: Omit<InputCategory, 'isGap'>[] = [];
  let fileInputIndex = 0;

  // Track which files we've already added
  const addedFiles = new Set<string>();
  // Track original file mappings for split segments
  const filePathToIndex = new Map<string, number>();

  inputs.forEach((input, originalIndex) => {
    const path = getInputPath(input);
    const trackInfo = getTrackInfo(input);
    const isGap = isGapInput(path);

    const isVideo =
      !isGap &&
      (FILE_EXTENSIONS.VIDEO.test(path) || FILE_EXTENSIONS.IMAGE.test(path));
    const isAudio = !isGap && FILE_EXTENSIONS.AUDIO.test(path);

    if (isVideo) {
      let fileIndex = -1;
      let audioFileIndex: number | undefined = undefined;

      if (!isGap) {
        // For non-gap files, assign file index
        if (!addedFiles.has(path)) {
          fileIndex = fileInputIndex;
          filePathToIndex.set(path, fileIndex);
          addedFiles.add(path);
          fileInputIndex++;
        } else {
          // Use the existing file index for this file
          fileIndex = filePathToIndex.get(path) ?? -1;
        }

        // If this video has a separate audio file, register it
        if (trackInfo.audioPath) {
          if (!addedFiles.has(trackInfo.audioPath)) {
            audioFileIndex = fileInputIndex;
            filePathToIndex.set(trackInfo.audioPath, audioFileIndex);
            addedFiles.add(trackInfo.audioPath);
            fileInputIndex++;
            console.log(
              `üéµ Registered audio file for video input ${originalIndex}: audio file index ${audioFileIndex}`,
            );
          } else {
            // Audio file was already added, get its index
            audioFileIndex = filePathToIndex.get(trackInfo.audioPath);
            console.log(
              `üéµ Reusing existing audio file index ${audioFileIndex} for video input ${originalIndex}`,
            );
          }
        }
      }

      // Create a modified trackInfo with the audio file index stored
      const modifiedTrackInfo =
        audioFileIndex !== undefined
          ? { ...trackInfo, audioFileIndex }
          : trackInfo;

      videoInputs.push({
        originalIndex,
        fileIndex,
        trackInfo: modifiedTrackInfo,
        isGap: false,
      });

      if (audioFileIndex !== undefined) {
        console.log(
          `üìπ Video input ${originalIndex}: video file index ${fileIndex}, audio file index ${audioFileIndex}`,
        );
      }
    } else if (isAudio) {
      let fileIndex = -1;

      if (!isGap) {
        if (!addedFiles.has(path)) {
          fileIndex = fileInputIndex;
          filePathToIndex.set(path, fileIndex);
          addedFiles.add(path);
          fileInputIndex++;
        } else {
          fileIndex = filePathToIndex.get(path) ?? -1;
        }
      }

      audioInputs.push({
        originalIndex,
        fileIndex,
        trackInfo,
      });
    } else if (isGap) {
      // Handle gap markers
      if (trackInfo.gapType === 'video' || trackInfo.trackType === 'video') {
        videoInputs.push({
          originalIndex,
          fileIndex: -1,
          trackInfo,
          isGap: true,
        });
      } else if (
        trackInfo.gapType === 'audio' ||
        trackInfo.trackType === 'audio'
      ) {
        audioInputs.push({
          originalIndex,
          fileIndex: -1,
          trackInfo,
        });
      }
    }
  });

  return { videoInputs, audioInputs, fileInputIndex };
}
// -------------------------
// Video Processing Functions
// -------------------------

/**
 * Creates video trimming filters for a given video track
 * Handles images differently (uses loop filter instead of trim)
 * @param context - Video processing context with track info and references
 * @returns Filter reference and filter strings for video trimming
 */
function createVideoTrimFilters(
  context: VideoProcessingContext,
): AudioTrimResult {
  const { trackInfo, originalIndex, inputStreamRef } = context;

  // Check if this is an image
  const isImage = trackInfo.isImage || trackInfo.trackType === 'image';

  if (isImage && trackInfo.duration !== undefined) {
    // For images, treat them like gaps but with the actual image content
    // Generate frames by looping the single image frame for the exact duration
    const trimmedRef = `[v${originalIndex}_trimmed]`;
    const fps = 30; // Default FPS, will be normalized later
    const duration = trackInfo.duration;
    const totalFrames = Math.round(duration * fps); // Round to nearest frame for exact timing

    console.log(
      `üñºÔ∏è Image input detected: generating ${duration}s (${totalFrames} frames) at ${fps}fps from static image`,
    );

    // ‚úÖ OPTIMIZATION: Don't apply timestamp filters on images (images don't carry PTS)
    // Use trim with exact duration - no setpts needed for static images
    return {
      filterRef: trimmedRef,
      filters: [`${inputStreamRef}trim=duration=${duration}${trimmedRef}`],
    };
  }

  // For videos, use standard trim
  if (trackInfo.startTime === undefined && trackInfo.duration === undefined) {
    // No trimming needed, just return the original reference
    return {
      filterRef: inputStreamRef,
      filters: [],
    };
  }

  const trimmedRef = `[v${originalIndex}_trimmed]`;
  let trimFilter = `${inputStreamRef}trim=`;

  // Build trim parameters
  const params = [];
  if (trackInfo.startTime !== undefined && trackInfo.startTime > 0) {
    params.push(`start=${trackInfo.startTime}`);
  }
  if (trackInfo.duration !== undefined) {
    params.push(`duration=${trackInfo.duration}`);
  }

  if (params.length > 0) {
    trimFilter += params.join(':') + `[temp_trim_${originalIndex}]`;
    return {
      filterRef: trimmedRef,
      filters: [
        trimFilter,
        `[temp_trim_${originalIndex}]setpts=PTS-STARTPTS${trimmedRef}`,
      ],
    };
  }

  return {
    filterRef: inputStreamRef,
    filters: [],
  };
}

/**
 * Creates gap video filters (black video generation)
 * @param originalIndex - Index of the gap input
 * @param duration - Duration of the gap
 * @param targetFps - Target frame rate
 * @returns Filter reference and filter strings for gap video
 */
function createGapVideoFilters(
  originalIndex: number,
  duration: number,
  targetFps: number,
  videoDimensions: { width: number; height: number },
): AudioTrimResult {
  const gapRef = `[gap_v${originalIndex}]`;
  return {
    filterRef: gapRef,
    filters: [
      `color=black:size=${videoDimensions.width}x${videoDimensions.height}:duration=${duration}:rate=${targetFps}[temp_gap_${originalIndex}]`,
      `[temp_gap_${originalIndex}]setpts=PTS-STARTPTS,setsar=1${gapRef}`,
    ],
  };
}

/**
 * Creates FPS normalization filters
 * @param originalIndex - Index of the input
 * @param inputRef - Input stream reference
 * @param targetFps - Target frame rate
 * @returns Filter reference and filter strings for FPS normalization
 */
function createFpsNormalizationFilters(
  originalIndex: number,
  inputRef: string,
  targetFps: number,
): AudioTrimResult {
  const fpsRef = `[v${originalIndex}_fps]`;
  return {
    filterRef: fpsRef,
    filters: [`${inputRef}fps=${targetFps}:start_time=0${fpsRef}`],
  };
}

/**
 * Creates SAR (Sample Aspect Ratio) normalization filters
 * Normalizes SAR to 1:1 to ensure compatibility with concat filter
 * @param originalIndex - Unique index for filter labeling
 * @param inputRef - Input filter reference (e.g., "[v0_trim]")
 * @returns AudioTrimResult with filter reference and filter strings
 */
function createSarNormalizationFilters(
  originalIndex: number,
  inputRef: string,
): AudioTrimResult {
  const sarRef = `[v${originalIndex}_sar]`;
  return {
    filterRef: sarRef,
    filters: [`${inputRef}setsar=1${sarRef}`],
  };
}

// -------------------------
// Audio Processing Functions
// -------------------------

/**
 * Creates audio trimming filters for a given audio track
 * @param context - Audio processing context with track info and references
 * @returns AudioTrimResult with filter reference and filter strings
 */
function createAudioTrimFilters(
  context: AudioProcessingContext,
): AudioTrimResult {
  const { trackInfo, originalIndex, inputStreamRef } = context;

  if (trackInfo.startTime === undefined && trackInfo.duration === undefined) {
    // No trimming needed, just reset timestamps
    const resetRef = `[a${originalIndex}_reset]`;
    return {
      filterRef: resetRef,
      filters: [`${inputStreamRef}asetpts=PTS-STARTPTS${resetRef}`],
    };
  }

  const trimmedRef = `[a${originalIndex}_trimmed]`;
  let trimFilter = `${inputStreamRef}atrim=`;

  // Build trim parameters
  const params = [];
  if (trackInfo.startTime !== undefined && trackInfo.startTime > 0) {
    params.push(`start=${trackInfo.startTime}`);
  }
  if (trackInfo.duration !== undefined) {
    params.push(`duration=${trackInfo.duration}`);
  }

  if (params.length > 0) {
    trimFilter += params.join(':') + `[temp_atrim_${originalIndex}]`;
    return {
      filterRef: trimmedRef,
      filters: [
        trimFilter,
        `[temp_atrim_${originalIndex}]asetpts=PTS-STARTPTS${trimmedRef}`,
      ],
    };
  }

  // Fallback: just reset timestamps
  const resetRef = `[a${originalIndex}_reset]`;
  return {
    filterRef: resetRef,
    filters: [`${inputStreamRef}asetpts=PTS-STARTPTS${resetRef}`],
  };
}

/**
 * Creates silent audio filters for gap inputs
 * @param originalIndex - Index of the gap input
 * @param duration - Duration of the silent audio
 * @returns AudioTrimResult with filter reference and filter strings
 */
function createSilentAudioFilters(
  originalIndex: number,
  duration: number,
): AudioTrimResult {
  const silentAudioRef = `[silent_a${originalIndex}]`;
  return {
    filterRef: silentAudioRef,
    filters: [
      `anullsrc=channel_layout=${AUDIO_DEFAULTS.CHANNEL_LAYOUT}:sample_rate=${AUDIO_DEFAULTS.SAMPLE_RATE}[temp_silent_pre_${originalIndex}]`,
      `[temp_silent_pre_${originalIndex}]atrim=duration=${duration}[temp_silent_${originalIndex}]`,
      `[temp_silent_${originalIndex}]asetpts=PTS-STARTPTS${silentAudioRef}`,
    ],
  };
}

// -------------------------
// Step handlers
// -------------------------

/**
 * Adds file inputs to the command
 * For video tracks with separate audio files, adds both the video and audio as inputs
 * Text tracks are excluded as they don't have file paths and are rendered via drawtext filters
 */
function handleFileInputs(job: VideoEditJob, cmd: CommandParts): void {
  const addedFiles = new Set<string>();

  job.inputs.forEach((input) => {
    const path = getInputPath(input);
    const trackInfo = getTrackInfo(input);

    // Skip gaps, text tracks (they have no file path), and empty paths
    if (
      isGapInput(path) ||
      trackInfo.trackType === 'text' ||
      !path ||
      path.trim() === ''
    ) {
      return;
    }

    // Add the main file (video or audio)
    if (!addedFiles.has(path)) {
      cmd.args.push('-i', escapePath(path));
      addedFiles.add(path);
      console.log(`üìÅ Added input file: ${path}`);
    }

    // If this is a video track with a separate audio file, add the audio file too
    if (trackInfo.audioPath && !addedFiles.has(trackInfo.audioPath)) {
      cmd.args.push('-i', escapePath(trackInfo.audioPath));
      addedFiles.add(trackInfo.audioPath);
      console.log(`üéµ Added separate audio input: ${trackInfo.audioPath}`);
    }
  });
}

/**
 * Applies FPS control to command output
 * This sets the output frame rate independently of the filter_complex fps normalization
 */
function handleOutputFps(job: VideoEditJob, cmd: CommandParts): void {
  if (job.operations.targetFrameRate) {
    cmd.args.push('-r', String(job.operations.targetFrameRate));
    console.log(`üéûÔ∏è Applied output FPS: ${job.operations.targetFrameRate}`);
  }
}

/**
 * Applies encoding settings to command
 */
function handleEncodingSettings(
  job: VideoEditJob,
  cmd: CommandParts,
  hwAccel: HardwareAcceleration | null,
): void {
  let videoCodec: string = ENCODING_DEFAULTS.VIDEO_CODEC;

  if (hwAccel) {
    // Use hardware codec
    videoCodec =
      job.operations.preferHEVC && hwAccel.hevcCodec
        ? hwAccel.hevcCodec
        : hwAccel.videoCodec;

    console.log(`üéÆ Using hardware video codec: ${videoCodec}`);
    console.log(
      `‚ö†Ô∏è  Note: If encoding fails, FFmpeg may not support this codec on your system.`,
    );
  } else {
    console.log(`üíª Using software video codec: ${videoCodec}`);
  }

  cmd.args.push('-c:v', videoCodec);
  cmd.args.push('-c:a', ENCODING_DEFAULTS.AUDIO_CODEC);

  // Add memory-efficient encoder settings
  if (!hwAccel) {
    // Software encoding: Add memory-efficient x264 settings
    console.log('üíæ Adding memory-efficient software encoder settings...');
    cmd.args.push(
      '-x264-params',
      'nal-hrd=cbr:force-cfr=1:rc-lookahead=10:bframes=0', // Reduce lookahead buffer, disable B-frames
    );
    console.log('   ‚úÖ Reduced x264 lookahead to 10 frames (default 40)');
    console.log('   ‚úÖ Disabled B-frames (reduces encoder buffering)');
  } else {
    // Hardware encoding: Add hardware encoder flags
    if (hwAccel.encoderFlags) {
      console.log(
        'üéÆ Adding hardware encoder flags:',
        hwAccel.encoderFlags.join(' '),
      );
      cmd.args.push(...hwAccel.encoderFlags);
    }
  }
}

/**
 * Applies encoding preset if specified (for software encoding only)
 */
function handlePreset(
  job: VideoEditJob,
  cmd: CommandParts,
  hwAccel: HardwareAcceleration | null,
): void {
  // Only apply software preset if not using hardware acceleration
  if (hwAccel) {
    console.log(
      '‚ÑπÔ∏è  Skipping software preset (using hardware encoder settings instead)',
    );
    return;
  }

  if (!job.operations.preset) return;

  cmd.args.push('-preset', job.operations.preset);
  cmd.args.push('-crf', '28');
  cmd.args.push('-b:a', '96k');

  console.log(`üöÄ Applied software encoding preset: ${job.operations.preset}`);
}

/**
 * Applies thread settings and faststart flag
 */
function handleThreads(job: VideoEditJob, cmd: CommandParts): void {
  cmd.args.push('-threads', String(job.operations.threads));

  console.log(`üöÄ Applied thread limit: ${job.operations.threads}`);
}

/**
 * Adds output file path to command
 */
function handleOutput(
  job: VideoEditJob,
  cmd: CommandParts,
  location?: string,
): void {
  const outputFilePath = location
    ? path.join(location, job.output)
    : job.output;

  cmd.args.push(outputFilePath);
}

// -------------------------
// Main builder
// -------------------------
export async function buildFfmpegCommand(
  job: VideoEditJob,
  location?: string,
  ffmpegPath?: string,
): Promise<string[]> {
  const cmd: CommandParts = { args: [], filters: [] };
  const targetFrameRate = job.operations.targetFrameRate || VIDEO_DEFAULTS.FPS;

  // Log aspect ratio from both sources
  console.log(
    'üìê ASPECT RATIO from job.operations.aspect:',
    job.operations.aspect || 'NONE',
  );

  // Check for aspect ratio in trackInfo
  let trackInfoAspectRatio: string | undefined = undefined;
  for (const input of job.inputs) {
    const trackInfo = getTrackInfo(input);
    if (trackInfo.aspectRatio) {
      trackInfoAspectRatio = trackInfo.aspectRatio;
      console.log(
        'üìê ASPECT RATIO from trackInfo.aspectRatio:',
        trackInfoAspectRatio,
      );
      break;
    }
  }
  if (!trackInfoAspectRatio) {
    console.log('üìê ASPECT RATIO from trackInfo.aspectRatio: NONE');
  }

  const hwAccel = await getHardwareAccelerationForJob(job, ffmpegPath);
  if (hwAccel) {
    console.log(
      `üéÆ Hardware acceleration detected: ${hwAccel.type.toUpperCase()}`,
    );
    console.log(
      '‚ÑπÔ∏è  Using hardware encoding only (software decoding for compatibility)',
    );
  }

  // Step 1: Add file inputs
  handleFileInputs(job, cmd);

  // Step 1.5: Add aggressive RAM optimization flags
  // These flags dramatically reduce memory usage at the cost of some processing speed
  console.log('üíæ Adding aggressive RAM optimization flags...');

  cmd.args.push(
    // === DECODER BUFFER LIMITS ===
    '-fflags',
    '+discardcorrupt+nobuffer', // Discard corrupt packets, disable input buffering
    '-flags',
    'low_delay', // Minimize decoder latency/buffering
    '-probesize',
    '5000000', // Limit input probing (5MB instead of default 5GB)
    '-analyzeduration',
    '5000000', // Limit stream analysis (5 seconds instead of default)

    // === FILTER GRAPH OPTIMIZATION ===
    '-filter_complex_threads',
    '1', // Sequential processing - CRITICAL for duplicate inputs

    // === OUTPUT BUFFER LIMITS ===
    '-max_muxing_queue_size',
    '512', // Aggressive limit on output queue (lower = less RAM)
    '-muxdelay',
    '0', // No muxing delay
    '-muxpreload',
    '0', // No muxing preload
  );

  console.log('   ‚úÖ Decoder buffering: DISABLED (nobuffer flag)');
  console.log('   ‚úÖ Probe size: LIMITED to 5MB (reduces initial RAM spike)');
  console.log(
    '   ‚úÖ Filter threads: 1 (sequential processing for duplicate inputs)',
  );
  console.log('   ‚úÖ Mux queue: 512 packets (aggressive RAM limit)');
  console.log(
    '   ‚ö†Ô∏è  Trade-off: Export will be slower but use 40-60% less RAM',
  );

  // Step 2: Build and process timelines with multi-layer support
  const { videoLayers, imageLayers, finalAudioTimeline, categorizedInputs } =
    handleTimelineProcessing(job, targetFrameRate, categorizeInputs);

  // Step 3: Build and apply filter complex with multi-layer compositing
  handleFilterComplex(
    job,
    cmd,
    videoLayers,
    imageLayers,
    finalAudioTimeline,
    categorizedInputs,
    hwAccel,
    createGapVideoFilters,
    createSilentAudioFilters,
    createAudioTrimFilters,
    createVideoTrimFilters,
    createFpsNormalizationFilters,
    createSarNormalizationFilters,
  );

  // Step 4: Apply encoding settings with hardware acceleration
  handleEncodingSettings(job, cmd, hwAccel);
  handlePreset(job, cmd, hwAccel);
  handleThreads(job, cmd);

  // Step 5: Apply aspect ratio and FPS controls
  handleOutputFps(job, cmd);

  // Step 6: Add output file
  handleOutput(job, cmd, location);

  console.log('Full FFmpeg Command:', ['ffmpeg', ...cmd.args].join(' '));
  return cmd.args;
}
